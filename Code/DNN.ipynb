{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e44b9-7b45-47a2-81a9-42fdf2cf17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc6657-5e3e-4be4-90be-535b5386f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "file_path = \"data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the last column is the target variable and all other columns are features\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f8d90-b61a-4cf7-afa5-a3895a7ef4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dropout(0.5, input_shape=(X.shape[1],)),\n",
    "    Dense(128, kernel_initializer='he_uniform', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, kernel_initializer='he_uniform', activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, kernel_initializer='he_uniform', activation='relu'),\n",
    "    Dense(16, kernel_initializer='he_uniform', activation='relu'),\n",
    "    Dense(8, kernel_initializer='he_uniform', activation='relu'),\n",
    "    Dense(1, kernel_initializer='he_uniform', activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, Y_train, epochs=250, batch_size=len(X_train), verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1103fdc0-0424-43ab-bf6a-3b55e3434c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Performance metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(Y_test, y_pred_classes)\n",
    "print(conf_matrix)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "print(\"True Negatives: \", TN)\n",
    "print(\"False Positives: \", FP)\n",
    "print(\"False Negatives: \", FN)\n",
    "print(\"True Positives: \", TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9b674-7985-4d88-9808-bc5e7327aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Precision, Recall, F1 Score, MCC, and ROC AUC\n",
    "accuracy = accuracy_score(Y_test, y_pred_classes)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "mcc = matthews_corrcoef(Y_test, y_pred_classes)\n",
    "roc_auc = roc_auc_score(Y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score))\n",
    "print(\"Matthews Correlation Coefficient: {:.2f}\".format(mcc))\n",
    "print(\"ROC AUC Score: {:.2f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330dd03-3f90-4253-8513-452a18b8c534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f516d-77f7-48d2-a6ee-cbd6899e4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training & validation accuracy and loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=3)\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy', linewidth=3)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d173b8-c3b6-4248-871f-0582c1c5ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the user's home directory\n",
    "home_dir = os.path.expanduser('~')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'], label='Train Loss', linewidth=3)\n",
    "plt.plot(history.history['val_loss'], label='Test Loss', linewidth=3)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss (%)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "#plt.title('Model Loss', fontsize=16)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(save_path, format='png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703f752-ba47-4a35-ad38-39fe5e6f4fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30a621-baf3-454f-8ff2-b8a4a450e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(Y_test, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d2eda-a557-4a5a-8a37-eafc2103b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'rf' is your trained Random Forest model and 'data' is your original dataset\n",
    "importance = rf.feature_importances_\n",
    "features = data.columns[:-1]  # Assuming data is your original dataset\n",
    "top_features_idx = np.argsort(importance)[-50:]  # Select top 50 features by importance\n",
    "top_features = features[top_features_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Use a different color for each bar\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_features_idx)))\n",
    "sns.barplot(x=importance[top_features_idx], y=top_features, palette=colors, ci=None)  # Set ci=None to remove confidence intervals\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Label each feature individually outside the bars\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.text(importance[top_features_idx[i]] + 0.0005, i, f'', va='center')\n",
    "\n",
    "# Add grid to the plot\n",
    "plt.grid(axis='both', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10417a-56db-4f10-a929-51a2001d3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'rf' is your trained Random Forest model, and 'X_train' and 'X_test' are your data splits\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=data.columns[:-1], mode=\"classification\")\n",
    "exp = explainer.explain_instance(X_test[10], rf.predict_proba, num_features=len(X[10]))\n",
    "\n",
    "# Generate the LIME explanation figure\n",
    "fig = exp.as_pyplot_figure()\n",
    "plt.title('LIME Explanation')\n",
    "\n",
    "# Extract feature names and their importance values from the LIME explanation\n",
    "feature_names = [feature[0] for feature in exp.as_list()]\n",
    "feature_importances = [feature[1] for feature in exp.as_list()]\n",
    "\n",
    "# Get the current axes and bars\n",
    "ax = plt.gca()\n",
    "bars = ax.patches\n",
    "\n",
    "# Label each feature individually inside the bars\n",
    "for bar, label in zip(bars, feature_names):\n",
    "    ax.text(bar.get_width() / 2, bar.get_y() + bar.get_height() / 2, label, ha='center', va='center', color='white')\n",
    "\n",
    "# Add x-axis and y-axis labels\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Features')\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True, linestyle='--', which='both', axis='both')\n",
    "\n",
    "# Remove y-axis labels\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9416302-fff5-4587-ab45-6a1177be9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your original dataset\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(data=data, orient='h')\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('Feature Distribution', fontsize=16)\n",
    "plt.xlabel('Feature Values', fontsize=14)\n",
    "plt.ylabel('Features', fontsize=14)\n",
    "\n",
    "# Center the title and labels\n",
    "plt.title('Feature Distribution', fontsize=16, loc='center')\n",
    "plt.xlabel('Feature Values', fontsize=14, labelpad=10)\n",
    "plt.ylabel('Features', fontsize=14, labelpad=10)\n",
    "\n",
    "# Adjust the tick parameters for better readability\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add a grid for better visual reference\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e4ec4-cbfc-4526-8ef8-99a59fbb8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'exp' is your LIME explanation object\n",
    "exp.as_pyplot_figure()\n",
    "plt.title('Local Contributions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167b15d-26f3-4b93-8936-9f3c7e3f1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'exp' is your LIME explanation object\n",
    "exp.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270dfd83-620f-4ce0-abcb-706496b2a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'y_pred' and 'Y_test' are your predictions and true labels\n",
    "errors = np.where(y_pred != Y_test)[0]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot predicted errors\n",
    "plt.scatter(range(len(errors)), y_pred[errors], marker='x', color='red', label='Predicted', s=100)\n",
    "\n",
    "# Plot true values where errors occurred\n",
    "plt.scatter(range(len(errors)), Y_test[errors], marker='o', color='blue', label='True', s=100)\n",
    "\n",
    "# Title and axis labels\n",
    "plt.title('Error Analysis', fontsize=16)\n",
    "plt.xlabel('Instance', fontsize=14)\n",
    "plt.ylabel('Prediction', fontsize=14)\n",
    "\n",
    "# Adding grid\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Legend and adjustments\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
